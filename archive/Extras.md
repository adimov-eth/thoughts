### New implementation‑relevant details that were **missing from the previous project documents**

| #   | Newly‑surfaced fact                                                                                                                                                                                                                                                                                                                                                                                                               | Why it matters / where it lives                                                                                                                          | Sources |
| --- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- |
| 1   | **Dual snapshot model.** State is persisted in two complementary column‑families in LevelDB: <br>• *mutable KV store* – current branch‑hashes keyed by the full prefix path (server → signer → entity → sub‑store). Written every _N_ blocks for fast cold‑start.<br>• *immutable CAS store* – content‑addressed, IPFS‑like blobs keyed by their hash; provides forensic replay of any past frame without ever being overwritten. | Establishes a clear recovery path (fast boot + audit‑grade history) and dictates how the storage layer must be wired.                                    |         |
| 2   | **Flat 96‑byte prefix key‑scheme.** During prototyping every record is addressed by the concatenation of three 32‑byte IDs (Signer ID ∥ Entity ID ∥ Store‑type). The root of the whole server is kept under the _empty_ key. This removes intermediate buckets and lets LevelDB range‑scans mirror the in‑memory map one‑to‑one.                                                                                                  | Simplifies serialisation (one RLP blob per node), eliminates extra look‑ups and keeps the debug‑UI trivial.                                              |         |
| 3   | **`frame` terminology.** All fast off‑chain blocks produced by _server_, _signer_, _entity_ and _channel_ machines are renamed to **frames**; only the slow on‑chain layer keeps the word _block_. Each frame header carries `{height, timestamp}`.                                                                                                                                                                               | Disambiguates UX and code: developers instantly see whether a structure is L2‑off‑chain or L1‑jurisdiction.                                              |         |
| 4   | **Deterministic key‑derivation at server boot.** A single 32‑byte `serverSecret` is expanded into an _unbounded_ list of signer key‑pairs (`signer[n] = H(serverSecret ∥ n)`); servers print the first _N_ public keys on start‑up so tests can refer to them by index.                                                                                                                                                           | Guarantees reproducible simulations and allows scripted genesis/import flows with zero extra configuration.                                              |         |
| 5   | **`input` vs `command` wire objects.** Every external packet is an RLP‑encoded **input**: `[targetSigner, targetEntity, command]`.<br>Inside the command the first field is its _type/kind_ (e.g. `importEntity`, `addTx`, `signFrame`). The server aggregates all inputs received during the current 100 ms tick into **one** `ServerInput` batch before processing.                                                             | Formalises the JSON/RLP schema that front‑end wallets and test scripts must emit; also fixes the “one‑batch‑per‑tick” rule used by the executor.         |         |
| 6   | **Event‑bus removed → pure function `apply()` model.** Instead of a central message‑bus, each machine exposes a single `apply(env, prefix, txs)` that mutates its slice _in‑place_; recursion down the prefix tree replaces pub/sub routing. All state lives in one shared `env.stateMap`.                                                                                                                                        | Cuts indirection, keeps replay simple (just re‑call `apply` in log order) and lets unit‑tests exercise sub‑machines directly.                            |         |
| 7   | **Channel state machine clarified.**<br>• Two transport states: **`ready`** (can propose) and **`send`** (waiting‑ack).<br>• A _channel frame_ itself _is_ the transaction; there is no secondary “tx pool”.<br>• Acknowledgement frame carries nothing but the BLS aggregate signature on the last hash.                                                                                                                         | Tells implementers exactly when they may emit a new frame and what minimal data an ack must include.                                                     |         |
| 8   | **Order‑book map inside Entity state.** Hubs keep a second KV‑sub‑store `orderBook` where the key is `hash(baseAsset ∥ quoteAsset ∥ direction ∥ marketType)` and the value is a compact list of open orders (`counterpartyEntity, amount/price`). Matching occurs **inside the entity frame** before channel messages are produced.                                                                                               | Introduces the missing data‑structure needed for spot swaps and makes it clear that matching is deterministic and on‑ledger, not in an external service. |         |
| 9   | **‘Replica’ concept for distributed Entities.** Each server stores _replicas_ of the Entities for which its signer is in the quorum. Replicas are indexed by `(signerID, entityID)`; the same Entity therefore appears on several servers under different signer prefixes but identical inner state hashes.                                                                                                                       | Clarifies how multi‑party governance is materialised in storage and why the target signer **must** be specified in every incoming message.               |         |
